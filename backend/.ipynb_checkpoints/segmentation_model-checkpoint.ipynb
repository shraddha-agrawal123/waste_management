{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff261c4a-3dbb-4d38-afa4-3cdbbaa052c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61c4052-45ed-4950-a75e-431c2f1815b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "image_dir = r\"C:\\Users\\shrad\\mini_project\\ai-waste-project\\backend\\datasets\\waste-images\"\n",
    "soil_csv_path = r\"C:\\Users\\shrad\\mini_project\\ai-waste-project\\backend\\datasets\\soil_nutrient_with_best.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32c9bd8-f4c2-4171-8bd5-5087ab3e706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load soil dataset\n",
    "soil_data = pd.read_csv(soil_csv_path)\n",
    "soil_data.columns = soil_data.columns.str.strip()  # Remove spaces in column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb11c1ae-969c-4d38-9e8a-3fd136fe4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil mapping keys: ['Chalky Soil', 'Chalky Soil  ', 'Clay Soil', 'Clay Soil  ', 'Loamy Soil', 'Loamy Soil  ', 'Peaty Soil', 'Peaty Soil  ', 'Saline Soil', 'Saline Soil  ', 'Sandy Soil', 'Sandy Soil  ', 'Silt Soil', 'Silt Soil  ']\n"
     ]
    }
   ],
   "source": [
    "# Ensure required columns exist\n",
    "required_columns = [\"Soil_Type\", \"Zinc(%)\", \"Copper(%)\", \"Iron(%)\", \"Nitrogen(%)\", \"Phosphorus(%)\", \"Potassium(%)\", \"Magnesium(%)\"]\n",
    "if not all(col in soil_data.columns for col in required_columns):\n",
    "    raise KeyError(f\"Missing required columns! Available columns: {soil_data.columns}\")\n",
    "\n",
    "# Convert nutrient columns to numeric\n",
    "for col in required_columns[1:]:  # Skip \"Soil_Type\"\n",
    "    soil_data[col] = pd.to_numeric(soil_data[col], errors=\"coerce\")  # Convert to numeric, set invalid values to NaN\n",
    "\n",
    "# Fill missing values with 0 (or handle them as needed)\n",
    "soil_data.fillna(0, inplace=True)\n",
    "\n",
    "# Group by Soil_Type and calculate mean nutrient values\n",
    "# Ensure only numeric columns are included in the mean calculation\n",
    "soil_data = soil_data.groupby(\"Soil_Type\")[required_columns[1:]].mean().reset_index()\n",
    "\n",
    "# Create soil mapping for lookup\n",
    "soil_mapping = soil_data.set_index(\"Soil_Type\").to_dict(orient=\"index\")\n",
    "\n",
    "# Print soil mapping keys\n",
    "print(\"Soil mapping keys:\", list(soil_mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09ee493-c286-44e8-88d2-a221199ea72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class WasteDataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, soil_mapping, batch_size=32, img_size=(224, 224), shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)  # Call the parent class constructor\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.soil_mapping = soil_mapping  # Soil nutrient mapping\n",
    "\n",
    "        # Initialize lists to store file paths and labels\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        self.class_indices = {}  # Map class names to indices\n",
    "        class_id = 0\n",
    "\n",
    "        # Iterate through each waste category folder\n",
    "        for category in os.listdir(image_dir):\n",
    "            category_path = os.path.join(image_dir, category)\n",
    "            if os.path.isdir(category_path):\n",
    "                if category not in self.class_indices:\n",
    "                    self.class_indices[category] = class_id\n",
    "                    class_id += 1\n",
    "                \n",
    "                # Add all images in the category folder to the list\n",
    "                for file in os.listdir(category_path):\n",
    "                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.image_files.append(os.path.join(category_path, file))\n",
    "                        self.labels.append(category)\n",
    "\n",
    "        self.num_classes = len(self.class_indices)  # Total number of waste categories\n",
    "        self.on_epoch_end()  # Shuffle data at initialization\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_files = self.image_files[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_soil_features = []\n",
    "\n",
    "        for file_path, label in zip(batch_files, batch_labels):\n",
    "            img = self.load_and_segment_image(file_path)\n",
    "            if img is None:\n",
    "                continue  # Skip if image failed to load\n",
    "\n",
    "            # Extract soil type from the filename (assuming format: soiltype_filename.jpg)\n",
    "            soil_type = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "            # Get soil features from soil_mapping\n",
    "            soil_features = self.soil_mapping.get(soil_type, np.zeros(7))\n",
    "            if isinstance(soil_features, dict):  # Check if soil_features is a dictionary\n",
    "                soil_features = list(soil_features.values())  # Convert dict values to list\n",
    "            elif isinstance(soil_features, np.ndarray):  # Check if soil_features is a numpy array\n",
    "                soil_features = soil_features.tolist()  # Convert numpy array to list\n",
    "\n",
    "            batch_images.append(img)\n",
    "            batch_soil_features.append(soil_features)\n",
    "\n",
    "        # Convert to TensorFlow tensors\n",
    "        batch_images = tf.convert_to_tensor(batch_images, dtype=tf.float32)\n",
    "        batch_soil_features = tf.convert_to_tensor(batch_soil_features, dtype=tf.float32)\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        batch_labels = [self.class_indices[label] for label in batch_labels]\n",
    "        batch_labels = tf.one_hot(batch_labels, depth=self.num_classes)\n",
    "\n",
    "        # Dummy nutrient outputs (since we don't have actual nutrient labels)\n",
    "        nutrient_outputs = tf.zeros((len(batch_labels), 7), dtype=tf.float32)\n",
    "\n",
    "        # Return inputs and outputs as tuples\n",
    "        return (batch_images, batch_soil_features), (batch_labels, nutrient_outputs)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.arange(len(self.image_files))\n",
    "            np.random.shuffle(indices)\n",
    "            self.image_files = [self.image_files[i] for i in indices]\n",
    "            self.labels = [self.labels[i] for i in indices]\n",
    "\n",
    "    def load_and_segment_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"❌ Failed to load image: {img_path}\")\n",
    "            return None\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        _, segmented = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "        img[segmented == 0] = 0  \n",
    "        \n",
    "        return img / 255.0  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ff6ba7-892d-47be-b1cc-fd156533a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 5s/step - loss: 1.2959 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.5651 - waste_class_loss: 1.2959 - val_loss: 12.2259 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.1808 - val_waste_class_loss: 12.2258\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 10s/step - loss: 0.5648 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.8172 - waste_class_loss: 0.5648 - val_loss: 2.7781 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.3510 - val_waste_class_loss: 2.7776\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4223s\u001b[0m 54s/step - loss: 0.4096 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.8586 - waste_class_loss: 0.4096 - val_loss: 7.8579 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.3502 - val_waste_class_loss: 7.8630\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 14s/step - loss: 0.3514 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.8948 - waste_class_loss: 0.3514 - val_loss: 16.7889 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.1698 - val_waste_class_loss: 16.7912\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2302s\u001b[0m 29s/step - loss: 0.2618 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.9126 - waste_class_loss: 0.2618 - val_loss: 12.6721 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.1994 - val_waste_class_loss: 12.6761\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1149s\u001b[0m 15s/step - loss: 0.2554 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.9220 - waste_class_loss: 0.2555 - val_loss: 26.5084 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.1626 - val_waste_class_loss: 26.5134\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1000s\u001b[0m 13s/step - loss: 0.1911 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.9430 - waste_class_loss: 0.1914 - val_loss: 21.9347 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.2264 - val_waste_class_loss: 21.9357\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1147s\u001b[0m 15s/step - loss: 0.2250 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.9321 - waste_class_loss: 0.2250 - val_loss: 9.3950 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.3751 - val_waste_class_loss: 9.3924\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8177s\u001b[0m 104s/step - loss: 0.1616 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.9440 - waste_class_loss: 0.1617 - val_loss: 18.8110 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.2584 - val_waste_class_loss: 18.8171\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1920s\u001b[0m 24s/step - loss: 0.1628 - nutrient_levels_loss: 0.0000e+00 - nutrient_levels_mae: 0.0000e+00 - waste_class_accuracy: 0.9488 - waste_class_loss: 0.1628 - val_loss: 21.0677 - val_nutrient_levels_loss: 0.0000e+00 - val_nutrient_levels_mae: 0.0000e+00 - val_waste_class_accuracy: 0.2062 - val_waste_class_loss: 21.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model training complete and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Initialize Generators\n",
    "image_dir = r\"C:\\Users\\shrad\\mini_project\\ai-waste-project\\backend\\datasets\\waste-images\\dataset-resized\"\n",
    "soil_mapping = soil_mapping  # Use the soil mapping created earlier\n",
    "\n",
    "batch_size = 32\n",
    "train_generator = WasteDataGenerator(image_dir, soil_mapping, batch_size=batch_size, shuffle=True)\n",
    "val_generator = WasteDataGenerator(image_dir, soil_mapping, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define Model\n",
    "image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
    "soil_input = Input(shape=(7,), name=\"soil_input\")  # Updated to 7 nutrient features\n",
    "\n",
    "# MobileNetV2 as base model\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "x = GlobalAveragePooling2D()(base_model(image_input))\n",
    "\n",
    "# Waste Classification Branch\n",
    "x_class = Dense(1024, activation=\"relu\")(x)\n",
    "waste_class_output = Dense(train_generator.num_classes, activation=\"softmax\", name=\"waste_class\")(x_class)\n",
    "\n",
    "# Nutrient Level Prediction Branch\n",
    "x_nutrient = Dense(256, activation=\"relu\")(soil_input)\n",
    "x_nutrient = Dense(128, activation=\"relu\")(x_nutrient)\n",
    "x_nutrient = Dense(64, activation=\"relu\")(x_nutrient)\n",
    "nutrient_output = Dense(7, activation=\"linear\", name=\"nutrient_levels\")(x_nutrient)  # Updated to 7 nutrient features\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[image_input, soil_input], outputs=[waste_class_output, nutrient_output])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss={\"waste_class\": \"categorical_crossentropy\", \"nutrient_levels\": \"mse\"},\n",
    "              metrics={\"waste_class\": \"accuracy\", \"nutrient_levels\": \"mae\"})\n",
    "\n",
    "# Train Model\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Save trained model\n",
    "model.save('backend/models/trashnet_mobilenetv2_nutrients.h5', save_format='h5')\n",
    "print(\"✅ Model training complete and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a15ce0-b5da-4bc9-ab22-560848919709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
